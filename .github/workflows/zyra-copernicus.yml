name: Zyra Copernicus Acquire

on:
  workflow_dispatch:
    inputs:
      DATASET_NAME:
        description: 'Dataset name (e.g., copernicus)'
        required: true
        type: string

env:
  ZYRA_SCHEDULER_IMAGE: ghcr.io/noaa-gsl/zyra-scheduler:latest

jobs:
  acquire-copernicus:
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/noaa-gsl/zyra-scheduler:latest
      options: >-
        --entrypoint ""
        --user 0
    env:
      COPERNICUS_USERNAME: ${{ secrets.COPERNICUS_USERNAME }}
      COPERNICUS_PASSWORD: ${{ secrets.COPERNICUS_PASSWORD }}

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: pip install --no-cache-dir 'zyra[datatransfer]' requests
      
      # --- UPDATED PYTHON SCRIPT BLOCK ---
      - name: Acquire imagery via Copernicus Data Space (OAuth2 + STAC v1)
        run: |
          python3 <<'PYCODE'
          import os, requests, json, sys
          from datetime import datetime, timedelta

          print("Starting Copernicus acquisition...")

          # --- Auth variables from environment ---
          try:
              CLIENT_ID = os.environ["CDSE_CLIENT_ID"]
              CLIENT_SECRET = os.environ["CDSE_CLIENT_SECRET"]
          except KeyError as e:
              print(f"❌ Error: Environment variable {e} is not set.")
              print("Please set CDSE_CLIENT_ID and CDSE_CLIENT_SECRET in your repo secrets.")
              sys.exit(1) # Fail the step

          # --- 1. Get OAuth2 token ---
          TOKEN_URL = "https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token"
          print("Requesting access token...")
          try:
              token_resp = requests.post(
                  TOKEN_URL,
                  data={
                      "grant_type": "client_credentials",
                      "client_id": CLIENT_ID,
                      "client_secret": CLIENT_SECRET,
                  },
                  timeout=60,
              )
              token_resp.raise_for_status()
              token = token_resp.json()["access_token"]
              headers = {"Authorization": f"Bearer {token}"}
              print("✅ Access token received.")
          
          except requests.exceptions.RequestException as e:
              print(f"❌ Error getting OAuth2 token: {e}")
              if hasattr(e, 'response') and e.response is not None:
                  print(f"Response details: {e.response.status_code} - {e.response.text}")
              sys.exit(1) # Fail the step

          # --- 2. Define search parameters ---
          # Use the robust POST /search endpoint
          STAC_SEARCH_URL = "https://stac.dataspace.copernicus.eu/v1/search"

          # --- TIME WINDOW CHANGE ---
          # Search a very wide window: the entire past year,
          # ending 10 days ago to avoid any indexing lag.
          end = datetime.utcnow() - timedelta(days=10)
          start = datetime.utcnow() - timedelta(days=375) # Approx 1 year + 10 days
          datetime_range = f"{start.strftime('%Y-%m-%dT%H:%M:%SZ')}/{end.strftime('%Y-%m-%dT%H:%M:%SZ')}"
          print(f"Searching 1-year window: {datetime_range}")

          # Widen the bounding box
          bbox = [9.0, 45.0, 13.0, 46.0]  # Broader Northern Italy region
          
          # Build the search payload for the POST request
          search_payload = {
              "collections": ["Sentinel-2-L2A"], 
              "bbox": bbox,
              "datetime": datetime_range,
              "limit": 5,
              
              # --- SYNTAX CHANGE ---
              # Replace the simple "query" block with the
              # STAC API Filter Extension syntax ("cql2-json").
              # This is the more robust, standard-compliant way
              # to filter by cloud cover and may be required by the API.
              "filter-lang": "cql2-json",
              "filter": {
                  "op": "<=",
                  "args": [
                      { "property": "eo:cloud_cover" },
                      90
                  ]
              }
              # -----------------------------
          }

          print(f"Querying STAC search endpoint: {STAC_SEARCH_URL}")
          print(f"Payload: {json.dumps(search_payload, indent=2)}")

          # --- 3. Query STAC endpoint ---
          try:
              # Use POST and send the parameters as a JSON payload
              resp = requests.post(STAC_SEARCH_URL, headers=headers, json=search_payload, timeout=120)
              
              if resp.status_code == 403:
                  print("❌ Forbidden (403): Check your client permissions for Catalogue API.")
                  print(f"Response: {resp.text}")
                  sys.exit(1) # Fail the step
                  
              resp.raise_for_status()
              data = resp.json()

          except requests.exceptions.RequestException as e:
              print(f"❌ Error querying STAC endpoint: {e}")
              if hasattr(e, 'response') and e.response is not None:
                  print(f"Response details: {e.response.status_code} - {e.response.text}")
              sys.exit(1) # Fail the step

          # --- 4. Process and Download Results ---
          features = data.get("features", [])
          print(f"✅ Found {len(features)} results.")

          # Get dataset name from env var passed to the step
          dataset_name = os.environ.get('DATASET_NAME', 'copernicus')
          outdir = f"_work/images/{dataset_name}"
          os.makedirs(outdir, exist_ok=True)
          print(f"Downloading files to: {os.path.abspath(outdir)}")

          if not features:
              print("No features found matching criteria. Job complete.")
              sys.exit(0) # Exit cleanly, not an error

          for f in features:
              title = f.get("id", "unknown_id")
              assets = f.get("assets", {})
              
              if not assets:
                  print(f"⚠️ No downloadable assets for {title}")
                  continue

              # Try to find a common/useful asset key, fallback to the first one
              asset_key = None
              if 'data' in assets:
                  asset_key = 'data'
              elif 'thumbnail' in assets:
                  asset_key = 'thumbnail'
              else:
                  asset_key = next(iter(assets)) # Get the first asset key

              url = assets[asset_key]["href"]
              print(f"⬇️  Downloading {title} (asset: '{asset_key}')...")
              
              try:
                  r2 = requests.get(url, headers=headers, stream=True, timeout=600)
                  
                  if r2.status_code != 200:
                      print(f"⚠️ Skipping {title}, received HTTP {r2.status_code}")
                      print(f"URL: {url}")
                      continue
                  
                  # Save with .zip extension to match the original workflow's intent,
                  # even if the asset isn't a zip. This ensures the upload-artifact
                  # step works as originally designed.
                  filename = f"{title}.zip"
                  dest = os.path.join(outdir, filename)
                  
                  with open(dest, "wb") as fz:
                      for chunk in r2.iter_content(chunk_size=8192):
                          fz.write(chunk)
                  print(f"   Saved to {dest}")

              except requests.exceptions.RequestException as e:
                  print(f"❌ Error downloading asset for {title}: {e}")
                  
          print("\n✅ Acquisition complete.")
          PYCODE
        env:
          CDSE_CLIENT_ID: ${{ secrets.CDSE_CLIENT_ID }}
          CDSE_CLIENT_SECRET: ${{ secrets.CDSE_CLIENT_SECRET }}
          DATASET_NAME: ${{ inputs.DATASET_NAME || 'copernicus' }}
      
      # --- This step remains unchanged ---
      - name: Upload imagery artifacts
        uses: actions/upload-artifact@v4
        with:
          name: copernicus-${{ inputs.DATASET_NAME }}
          path: _work/images/${{ inputs.DATASET_NAME }}
