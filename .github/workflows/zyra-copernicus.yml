name: Zyra Copernicus Acquire

on:
  workflow_dispatch:
    inputs:
      DATASET_NAME:
        description: Dataset name (e.g., copernicus)
        required: true
        type: string

env:
  ZYRA_SCHEDULER_IMAGE: ghcr.io/noaa-gsl/zyra-scheduler:latest

jobs:
  acquire-copernicus:
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/noaa-gsl/zyra-scheduler:latest
      options: >-
        --entrypoint ""
        --user 0
    env:
      COPERNICUS_USERNAME: ${{ secrets.COPERNICUS_USERNAME }}
      COPERNICUS_PASSWORD: ${{ secrets.COPERNICUS_PASSWORD }}

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: pip install --no-cache-dir 'zyra[datatransfer]' requests
      
      - name: Acquire imagery via Copernicus Data Space (OAuth2 + STAC)
        run: |
          python3 <<'PYCODE'
          import os, requests, json
          from datetime import datetime, timedelta
      
          # Load OAuth credentials from secrets
          CLIENT_ID = os.environ["CDSE_CLIENT_ID"]
          CLIENT_SECRET = os.environ["CDSE_CLIENT_SECRET"]
      
          # OAuth2 token endpoint
          TOKEN_URL = "https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token"
      
          # Step 1: Request access token
          print("Requesting access token...")
          token_resp = requests.post(
              TOKEN_URL,
              data={
                  "grant_type": "client_credentials",
                  "client_id": CLIENT_ID,
                  "client_secret": CLIENT_SECRET,
              },
              timeout=60,
          )
          token_resp.raise_for_status()
          token = token_resp.json()["access_token"]
          headers = {"Authorization": f"Bearer {token}"}
      
          # Step 2: Query STAC API for Sentinel data
          STAC_URL = "https://catalogue.dataspace.copernicus.eu/stac/collections/SENTINEL-2/items"

          # For Sentinel-2, use SENTINEL-2 instead of SENTINEL-1
      
          end = datetime.utcnow()
          start = end - timedelta(days=30)
          bbox = [10.0, 45.0, 12.0, 47.0]  # Northern Italy

      
          params = {
              "bbox": ",".join(map(str, bbox)),
              "datetime": f"{start.isoformat()}Z/{end.isoformat()}Z",
              "limit": 5,
              "query": json.dumps({"eo:cloud_cover": {"lt": 20}})
          }

      
          print("Querying STAC endpoint...")
          resp = requests.get(STAC_URL, headers=headers, params=params, timeout=120)
          if resp.status_code == 403:
              print("❌ Forbidden: Check that your client ID/secret has catalogue access.")
              print(resp.text)
              exit(1)
          resp.raise_for_status()
          data = resp.json()
      
          features = data.get("features", [])
          print(f"Found {len(features)} results")
      
          outdir = f"_work/images/{os.environ.get('DATASET_NAME', 'copernicus')}"
          os.makedirs(outdir, exist_ok=True)
      
          for f in features:
              title = f["id"]
              # Find an asset with downloadable data (some STAC items have multiple assets)
              asset = next(iter(f["assets"].values()))
              url = asset["href"]
              print(f"Downloading {title} → {url}")
              r2 = requests.get(url, headers=headers, stream=True, timeout=300)
              r2.raise_for_status()
              with open(os.path.join(outdir, f"{title}.zip"), "wb") as fp:
                  for chunk in r2.iter_content(chunk_size=8192):
                      fp.write(chunk)
      
          print("✅ Acquisition complete.")
          PYCODE
        env:
          CDSE_CLIENT_ID: ${{ secrets.CDSE_CLIENT_ID }}
          CDSE_CLIENT_SECRET: ${{ secrets.CDSE_CLIENT_SECRET }}
          DATASET_NAME: ${{ inputs.DATASET_NAME || 'copernicus' }}

      - name: Upload imagery artifacts
        uses: actions/upload-artifact@v4
        with:
          name: copernicus-${{ inputs.DATASET_NAME }}
          path: _work/images/${{ inputs.DATASET_NAME }}
